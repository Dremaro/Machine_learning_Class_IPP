{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gwxqGln8dEK"
      },
      "source": [
        "# IMA 205 - TP ANN (part A)\n",
        "## Coding a Multi-Layer Perceptron in python (for a binary classification problem)\n",
        "\n",
        "*TP adapted by Alasdair Newson from the IA-306 course, originally written by Geoffroy Peeters*<br>\n",
        "\n",
        "Modified by Loic Le Folgoc. If you have questions, contact me at loic.lefolgoc@telecom-paris.fr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztxHJzckLfvM"
      },
      "source": [
        "$\\newcommand{\\underbr}[2]{\\underbrace{#1}_{\\scriptscriptstyle{#2}}}$\n",
        "\n",
        "### Objective:\n",
        "We want to implement a 1 hidden layer MLP in Python.\n",
        "\n",
        "#### Forward propagation\n",
        "\n",
        "- $\\large \\underbr{Z^{[1]}}{(n^{[1]},m)} = \\underbr{W^{[1]}}{(n^{[1]},n^{[0]})} \\underbr{X}{(n^{[0]},m)} + \\underbr{b^{[1]}}{n^{(1)}} $\n",
        "- $\\large \\underbr{A^{[1]}}{(n^{[1]},m)} = f(Z^{[1]})$\n",
        "- $\\large \\underbr{Z^{[2]}}{(n^{[2]},m)} = \\underbr{W^{[2]}}{(n^{[2]},n^{[1]})} \\underbr{A^{[1]}}{(n^{[1]},m)} + \\underbr{b^{[2]}}{n^{(2)}}$\n",
        "- $\\large \\underbr{A^{[2]}}{(n^{[2]},m)} = \\sigma(Z^{[2]})$\n",
        "\n",
        "where:\n",
        "- $\\hat{y} = \\large A^{[2]}$, in other words $\\large A^{[2]}$ is the network output\n",
        "- $f$ is a ```Relu``` function (the code is provided)\n",
        "- $\\sigma$ is a sigmoid function (the code is provided)\n",
        "- $m$ is the minibatch size (number of data samples in the minibatch)\n",
        "- $n^{[0]}$ is the number of neurons in the input layer (dimensionality of a data sample)\n",
        "- $n^{[1]}$ is the number of neurons in the hidden layer\n",
        "- $n^{[2]}=1$ is the number of neurons in the output layer (dimensionality of the output =1 for binary classification)\n",
        "\n",
        "#### Compute the cost\n",
        "\n",
        "The cost is average of the the loss over the training data. Since we are dealing with a binary classification problem, we will use the binary cross-entropy, which for 1 data sample writes as:\n",
        "\n",
        "$\\mathcal{L}(\\hat{y},y) = - y \\log(\\hat{y}) - (1-y) \\log(1-\\hat{y}) $\n",
        "\n",
        "where $y$ is the label (ground truth) and $\\hat{y}$ is the prediction.\n",
        "\n",
        "#### Backward propagation\n",
        "\n",
        "We admit that the backward propagation can be calculated as follows. We note $dZ$ as a shortcut for $\\frac{\\partial \\mathcal{L}}{\\partial Z}$, and similarly for other variables.\n",
        "\n",
        "- $\\large \\underbr{dZ^{[2]}}{(n^{[2]},m)} = \\underbr{A^{[2]}}{(n^{[2]},m)} - \\underbr{{Y}}{(n^{[2]},m)}$\n",
        "- $\\large \\underbr{dW^{[2]}}{(n^{[2]},n^{[1]})} = \\frac{1}{m} \\underbr{dZ^{[2]}}{(n^{[2]},m)} {\\underbr{A^{[1]}}{(n^{[1]},m)}}^{T}$\n",
        "- $\\large \\underbr{db^{[2]}}{(n^{[2]})} = \\frac{1}{m} \\sum_{i=1}^{m} \\underbr{dZ^{[2]}}{(n^{[2]},m)}$\n",
        "\n",
        "- $\\large \\underbr{dA^{[1]}}{(n^{[1]},m)} = {\\underbr{W^{[2]}}{(n^{[2]},n^{[1]})}}^{T} \\underbr{dZ^{[2]}}{(n^{[2]},m)}$\n",
        "- $\\large \\underbr{dZ^{[1]}}{(n^{[1]},m)} = \\underbr{dA^{[1]}}{(n^{[1]},m)} \\: \\odot \\: f' (\\underbr{Z^{[1]}}{(n^{[1]},m)})$\n",
        "- $\\large \\underbr{dW^{[1]}}{(n^{[1]},n^{[0]})} = \\frac{1}{m} \\underbr{dZ^{[1]}}{(n^{[1]},m)} {\\underbr{X}{(n^{[0]},m)}}^{T}$\n",
        "- $\\large \\underbr{db^{[1]}}{(n^{[1]})} = \\frac{1}{m} \\sum_{i=1}^{m} \\underbr{dZ^{[1]}}{(n^{[1]},m)}$\n",
        "\n",
        "Write the corresponding backward propagation algorithm.\n",
        "\n",
        "#### Parameters update\n",
        "\n",
        "- Implement a simple gradient descent:\n",
        "- $W = W - \\alpha dW$\n",
        "\n",
        "\n",
        "#### IMPORTANT IMPLEMENTATION INFORMATION !\n",
        "\n",
        "We recall that the $\\odot$ operator refers to the __point-wise multiplication__ operation. Note that we need $f'$, the derivative of the ReLU function, in order to calculate $dZ^{(1)}$. You will need to specify this in the ```F_dRelu``` function below. Finally, note that the __matrix multiplication__ operation, on the other hand, can be carried out in Python using ```np.dot(.,.)``` function.\n",
        "\n",
        "__The ```keepdims``` option__: In numpy, vectors and matrices are treated quite differently. When we mix them up, sometimes errors occur, even if mathematically speaking the operations should work. In particular, when we calculate the sum or the mean along one dimension of a matrix, the result is a vector. However, depending on which dimension this is, the result could be a column or row vector. In numpy, the second axis is not kept: the result is converted into a vector (which does not have a second axis). Numpy does not distinguish between row and column vectors in this case. Thus, if you want numpy to keep the second dimension, you can add ```keepdims=True``` to the sum or mean operation. This may help you avoid errors.\n",
        "\n",
        "\n",
        "\n",
        "### Your task:\n",
        "\n",
        "You need to add the missing parts in the code (parts between ```# --- START CODE HERE``` and ```# --- END CODE HERE```)\n",
        "\n",
        "### Note\n",
        "\n",
        "The code is written using python classes (in order to be able to pass all the variables easily from one function to the other).\n",
        "\n",
        "To use a given variable, you need to use ```self.$VARIABLE_NAME```, such as````self.W1````,```self.b1```, ... (see the code already written).\n",
        "\n",
        "### Testing\n",
        "\n",
        "For testing your code, you can use the code provided in the last cells (loop over epochs and display of the loss decrease).\n",
        "You should obtain a cost which decreases over epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Slo71LLqLfvQ"
      },
      "source": [
        "# 1. Load packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2iYj44p2LfvR"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn import model_selection\n",
        "import matplotlib.pyplot as plt\n",
        "import pdb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zha1equyLfvS"
      },
      "source": [
        "# 2. Define a set of functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtaPUpgpLfvS"
      },
      "outputs": [],
      "source": [
        "def F_standardize(X):\n",
        "    \"\"\"\n",
        "    standardize X, i.e. subtract mean (over data) and divide by standard-deviation (over data)\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X: np.array of size (nbData, nbDim)\n",
        "        matrix containing the observation data\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    X: np.array of size (nbData, nbDim)\n",
        "        standardize version of X\n",
        "    \"\"\"\n",
        "\n",
        "    X -= np.mean(X, axis=0, keepdims=True)\n",
        "    X /= (np.std(X, axis=0, keepdims=True) + 1e-16)\n",
        "    return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uM5rOnjwLfvT"
      },
      "outputs": [],
      "source": [
        "def F_sigmoid(x):\n",
        "    \"\"\"Compute the value of the sigmoid activation function\"\"\"\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def F_relu(x):\n",
        "    \"\"\"Compute the value of the Rectified Linear Unit activation function\"\"\"\n",
        "    return x * (x > 0)\n",
        "\n",
        "def F_dRelu(x):\n",
        "    \"\"\"Compute the derivative of the Rectified Linear Unit activation function\"\"\"\n",
        "    y = x\n",
        "    ## --- START CODE HERE\n",
        "    y = ...\n",
        "    # --- END CODE HERE\n",
        "    return y\n",
        "\n",
        "def F_computeCost(hat_y,y):\n",
        "    \"\"\"Compute the cost (sum of the losses)\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    hat_y: (1, nbData)\n",
        "        predicted value by the MLP\n",
        "    y: (1, nbData)\n",
        "        ground-truth class to predict\n",
        "    \"\"\"\n",
        "    m = hat_y.shape[1]\n",
        "\n",
        "    # --- START CODE HERE\n",
        "    loss = ...\n",
        "    # --- END CODE HERE\n",
        "\n",
        "    cost = np.sum(loss) / m\n",
        "    return cost\n",
        "\n",
        "def F_computeAccuracy(hat_y,y):\n",
        "    \"\"\"Compute the accuracy\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    hat_y: (1, nbData)\n",
        "        predicted value by the MLP\n",
        "    y: (1, nbData)\n",
        "        ground-truth class to predict\n",
        "    \"\"\"\n",
        "\n",
        "    m = y.shape[1]\n",
        "    class_y = np.copy(hat_y)\n",
        "    class_y[class_y>=0.5]=1\n",
        "    class_y[class_y<0.5]=0\n",
        "    return np.sum(class_y==y) / m"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rh5awVpLfvT"
      },
      "source": [
        "# 3. Load dataset and pre-process it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZSG7eWrLfvU"
      },
      "outputs": [],
      "source": [
        "X, y = datasets.make_circles(n_samples=1000, noise=0.05, factor=0.5)\n",
        "\n",
        "print(\"X.shape: {}\".format(X.shape))\n",
        "print(\"y.shape: {}\".format(y.shape))\n",
        "print(set(y))\n",
        "\n",
        "# X is (nbExamples, nbDim)\n",
        "# y is (nbExamples,)\n",
        "\n",
        "# --- Standardize data\n",
        "X = F_standardize(X)\n",
        "\n",
        "# --- Split between training set and test set\n",
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# --- Convert to proper shape: (nbExamples, nbDim) -> (nbDim, nbExamples)\n",
        "X_train = X_train.T\n",
        "X_test = X_test.T\n",
        "\n",
        "# --- Convert to proper shape: (nbExamples,) -> (1, nbExamples)\n",
        "y_train = y_train.reshape(1, len(y_train))\n",
        "y_test = y_test.reshape(1, len(y_test))\n",
        "\n",
        "n_in = X_train.shape[0]\n",
        "n_out = 1\n",
        "\n",
        "print(\"X_train.shape: {}\".format(X_train.shape))\n",
        "print(\"X_test.shape: {}\".format(X_test.shape))\n",
        "print(\"y_train.shape: {}\".format(y_train.shape))\n",
        "print(\"y_test.shape: {}\".format(y_test.shape))\n",
        "print(\"n_in: {} n_out: {}\".format(n_in, n_out))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_08yUNIuLfvU"
      },
      "source": [
        "Now, let's visualise the data we are working on"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxzulpTaLfvV"
      },
      "outputs": [],
      "source": [
        "# plot data\n",
        "plt.scatter(X_train[0,np.ravel(y_train==0)],X_train[1,np.ravel(y_train==0)],color='r')\n",
        "plt.scatter(X_train[0,np.ravel(y_train==1)],X_train[1,np.ravel(y_train==1)],color='b')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqX9yiskLfvV"
      },
      "source": [
        "# 4. Define the MLP class with forward, backward and update methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9608LgHLfvW"
      },
      "outputs": [],
      "source": [
        "class C_MultiLayerPerceptron:\n",
        "    \"\"\"\n",
        "    A class used to represent a Multi-Layer Perceptron with 1 hidden layers\n",
        "\n",
        "    ...\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    W1, b1, W2, b2:\n",
        "        weights and biases to be learnt\n",
        "    Z1, A1, Z2, A2:\n",
        "        values of the internal neurons to be used for backpropagation\n",
        "    dW1, db1, dW2, db2, dZ1, dZ2:\n",
        "        partial derivatives of the loss w.r.t. parameters\n",
        "\n",
        "    Methods\n",
        "    -------\n",
        "    forward_propagation\n",
        "\n",
        "    backward_propagation\n",
        "\n",
        "    update_parameters\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    W1, b1, W2, b2 = [], [], [], []\n",
        "    Z1, A1, Z2, A2 = [], [], [], []\n",
        "    dW1, db1, dW2, db2 = [], [], [], []\n",
        "    dZ1, dA1, dZ2 = [], [], []\n",
        "\n",
        "    def __init__(self, n_in, n_h, n_out):\n",
        "        #initialise weight and biases parameters\n",
        "        self.W1 = np.random.randn(n_h, n_in) * 0.01\n",
        "        self.b1 = np.zeros(shape=(n_h, 1))\n",
        "        self.W2 = np.random.randn(n_out, n_h) * 0.01\n",
        "        self.b2 = np.zeros(shape=(n_out, 1))\n",
        "        return\n",
        "\n",
        "\n",
        "    def __setattr__(self, attrName, val):\n",
        "        if hasattr(self, attrName):\n",
        "            self.__dict__[attrName] = val\n",
        "        else:\n",
        "            raise Exception(\"self.%s note part of the fields\" % attrName)\n",
        "\n",
        "\n",
        "\n",
        "    def M_forwardPropagation(self, X):\n",
        "        \"\"\"Forward propagation in the MLP\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: numpy array (nbDim, nbData)\n",
        "            observation data\n",
        "\n",
        "        Return\n",
        "        ------\n",
        "        hat_y: numpy array (1, nbData)\n",
        "            predicted value by the MLP\n",
        "        \"\"\"\n",
        "\n",
        "        # --- START CODE HERE\n",
        "        self.Z1 = ...\n",
        "        self.A1 = ...\n",
        "        self.Z2 = ...\n",
        "        self.A2 = ...\n",
        "        # --- END CODE HERE\n",
        "\n",
        "        hat_y = self.A2\n",
        "\n",
        "        return hat_y\n",
        "\n",
        "\n",
        "    def M_backwardPropagation(self, X, y):\n",
        "        \"\"\"Backward propagation in the MLP\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: numpy array (nbDim, nbData)\n",
        "            observation data\n",
        "        y: numpy array (1, nbData)\n",
        "            ground-truth class to predict\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        m = y.shape[1]  #batch size\n",
        "\n",
        "        # --- START CODE HERE\n",
        "        self.dZ2 = ...\n",
        "        self.dW2 = ...\n",
        "        self.db2 = ...\n",
        "        self.dA1 = ...\n",
        "        self.dZ1 = ...\n",
        "        self.dW1 = ...\n",
        "        self.db1 = ...\n",
        "        # --- END CODE HERE\n",
        "        return\n",
        "\n",
        "\n",
        "    def M_gradientDescent(self, alpha):\n",
        "        \"\"\"Update the parameters of the network using gradient descent\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        alpha: float scalar\n",
        "            amount of update at each step of the gradient descent\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # --- START CODE HERE\n",
        "        self.W1 = ...\n",
        "        self.b1 = ...\n",
        "        self.W2 = ...\n",
        "        self.b2 = ...\n",
        "        # --- END CODE HERE\n",
        "\n",
        "        return\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ldd6E2HLfvb"
      },
      "source": [
        "# 5. Perform training using batch-gradient and epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bG25KAEGLfvf"
      },
      "outputs": [],
      "source": [
        "# Instantiate the class MLP with providing\n",
        "# the size of the various layers (input=4, hidden=10, outout=1)\n",
        "\n",
        "n_hidden = 10\n",
        "num_epoch = 5000\n",
        "\n",
        "\n",
        "myMLP = C_MultiLayerPerceptron(n_in, n_hidden, n_out)\n",
        "\n",
        "train_cost, train_accuracy, test_cost, test_accuracy = [], [], [], []\n",
        "\n",
        "# Run over epochs\n",
        "for i in range(0, num_epoch):\n",
        "\n",
        "    # --- Forward\n",
        "    y_predict_train = myMLP.M_forwardPropagation(X_train)\n",
        "\n",
        "    # --- Store results on train\n",
        "    train_cost.append( F_computeCost(y_predict_train, y_train) )\n",
        "    train_accuracy.append( F_computeAccuracy(y_predict_train, y_train) )\n",
        "\n",
        "    # --- Backward\n",
        "    myMLP.M_backwardPropagation(X_train, y_train)\n",
        "\n",
        "    # --- Update\n",
        "    myMLP.M_gradientDescent(alpha=0.1)\n",
        "\n",
        "    # --- Store results on test\n",
        "    y_predict_test = myMLP.M_forwardPropagation(X_test)\n",
        "    test_cost.append( F_computeCost(y_predict_test, y_test) )\n",
        "    test_accuracy.append( F_computeAccuracy(y_predict_test, y_test) )\n",
        "\n",
        "    if (i % 100)==0:\n",
        "        print(\"epoch: {0:d} (cost: train {1:.2f} test {2:.2f}) (accuracy: train {3:.2f} test {4:.2f})\".format(i, train_cost[-1], test_cost[-1], train_accuracy[-1], test_accuracy[-1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBtGFiq8Lfvf"
      },
      "source": [
        "## Display train/test loss and accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zr-YfwrSLfvg"
      },
      "outputs": [],
      "source": [
        "plt.subplot(1,2,1)\n",
        "plt.plot(train_cost, 'r')\n",
        "plt.plot(test_cost, 'g--')\n",
        "plt.xlabel('# epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.grid(True)\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(train_accuracy, 'r')hat_\n",
        "plt.plot(test_accuracy, 'g--')\n",
        "plt.xlabel('# epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.grid(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMobHJ9tLfvg"
      },
      "outputs": [],
      "source": [
        "train_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFTUIfjiLfvg"
      },
      "outputs": [],
      "source": [
        "test_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1zgogk7Lil1"
      },
      "source": [
        "Tensorflow playground\n",
        "You can check out the tensorflow Playground here to test out an MLP graphically :\n",
        "\n",
        "https://playground.tensorflow.org/"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}